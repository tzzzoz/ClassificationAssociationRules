<HTML>
<HEAD>
<TITLE>CPAR</TITLE>
</HEAD>
<BODY BGCOLOR="white">
<CENTER><TABLE BORDER = 0 cellpadding = 10 WIDTH="100%">
<TR><TD BGCOLOR = BB0000>
<CENTER><TABLE BORDER = 0 cellpadding = 10 WIDTH="90%">
<TR><TD BGCOLOR = 006699>
<FONT COLOR = "white">
<CENTER>
<H1>THE LUCS-KDD IMPLEMENTATIONS OF CPAR (CLASSIFICATION BASED ON 
PREDICTIVE ASSOCIATION RULES)</H1>
<HR WIDTH=<50%">
<BR></CENTER>
<TABLE ALIGN=right BGCOLOR="white" BORDER=1 CELLPADDING=5>
<TR><TD>
<img src="../../../GifFolder/logo115.gif" 
	alt="Liverpool University">
</TD></TABLE>
<P><B><I>Frans Coenen<I></B></P>
<P><B>Department of Computer Science</B></P>
<P><B>The University of Liverpool</B></P>
<P>16 February 2004</P>
</FONT></TD>
</TABLE></CENTER>
</TD></TABLE>
</CENTER>

<P><B>DISCLAIMER!</B> The following description of the CPAR algorithm 
(Yin and Han 2003) is that
implemented by the author of this WWW page, i.e. it may not be identical
to that first produced by Xiaoxin Yin and Jiawei Han but certainly displays 
all the "salient"
features of the CPAR algorithm.</P> 

<H2>CONTENTS</H2>

<TABLE BORDER=0 CELLPADDING=0 WIDTH=100%>
<TR><TD WIDTH="48%">
<DL>
<DT>1. <A HREF = "#overview">Overview of CPAR algorithm</A>.</DT>
<DT>2. <A HREF = "#pruning">Rule pruning</A>.</DT>
</DL>
</TD><TD><PRE> </PRE></TD><TD>
<DL>
<DT>3. <A HREF = "#testing">Testing</A>.</DT>
<DT>4. <A HREF = "#example">Example</A>.</DT>
</DL>
</TD></TABLE>

<P><B>NOTE:</B> The description given here should be read in the context of; 
(a)  the <A HREF = "foil.html">FOIL</A> (First Order Inductive Learner) 
algorithm developed by Quinlan and Cameron-Jones, and (b) the
<A HREF = "prm.html">PRM</A> (Predictive Rule Mining) algorithm 
also developed by Xiaoxin Yin and Jiawei Han.</P>

<BR><HR><BR>
<A NAME = "overview">
<TABLE BORDER-=0 WIDTH="100%" CELLPADDING=5 BGCOLOR="006699">
<TR><TD><H2><font color = "white">1. OVERVIEW OF CPAR ALGORITHM</font></H2></TD>
</TABLE>

<P>The CPAR (Classification based on Predictive Association Rules) algorithm 
(Xiaoxin Yin and Jiawei Han) is an extension of PRM which in turn is an
extension of FOIL
(Quinlan and Cameron-Jones 1993). The distinction between CPAR and PRM
is that instead of choosing only the attribute that displays the best
gain on each iteration (as in FOIL and PRM), CPAR may choose a number of
attributes if those attributes have similar best gain. This is done by first
calculating the gain and applying a <TT>GAIN_SIMILARITY_RATIO</TT> to this 
(Xiaoxin Yin and Jiawei Han suggest 0.99). All attributes with gain better 
than:</P>

<PRE>
bestGain*GAIN_SIMILARITY_RATIO
</PRE>

<P>are then selected for further processing.</P> 

<P>CPAR (like FOIL and PRM) takes as input a (space separated) <I>binary 
valued</I> data set <TT>R</TT> and produces a set of CARs. The resulting
classifier by the LUCS-KDD PRM algorithm comprises a linked-list of rules 
ordered according to Laplace accuracy (Clark and Boswell 1991) of individual 
rules.</P>

<P>The CPAR algorithm, as implemented by the LUCS-KDD research team,
is presented in Tables 1 and 2.</P>

<CENTER><TABLE BORDER=1 CELLPADDING="10"><TR><TD>
<PRE>
method: startCPAR
Parameters: none
Global access to: R, C
----------------------------------------------

generate an empty global attributes array A

For each c in C
    generate global P and N example arrays
    generate global PN array
    determine minimum total Weight threshold

    while (total weight P > minimum total Weight threshold)
	A' <-- A, N' <-- N, P' <-- P, PN' <-- PN
	if no attributes exist with weightings that can
			produce a gain above minimum break
	cparGeneration({},c)
    end loop
end loop
</PRE>
</TD></TABLE>
<P><B>Table 1</B>: <I>The</I> <TT>startCPAR</TT> <I>method.</I></P>
</CENTER>
<CENTER><TABLE BORDER=1 CELLPADDING="10"><TR><TD>
<PRE>
method: cparGenerastion/2
Parameters: parameters ante (antecedent so far) and
	cons (consequent) for current rule.
Global access to: A, A', N, N', P, P', PN, PN'
----------------------------------------------

for each a in A
    if (a not in antecedent<SUP>1</SUP>) calculated gain using
    	       information in PN array and add to attribute array
end loop
i = "available" column in A with best gain
if (A[i][0] <= MIN_BEST_GAIN) return
    
loop through attribute array and find attribute a' with best gain
if (best gain <= MIN_BEST_GAIN) {
    add antecedent->c to rule list
    for all records in P reduce weighting by decay factor
     		and adjust PN array accordingly
    return

gainThreshold = bestGain*GAIN_SIMILARITY_RATIO
for each a in A
    if a available<SUP>1</SUP> and a.gain>gainThreshold 
	tempP' <-- P', tempN' <-- N', tempA' <-- A', tempNP'<--NP'
    	add a' to antecedent 
    	remove examples from N' and P' that do not contain 
		antecedent and adjust NP array accordingly
    	if (N' == {}) 
            add antecedent->c to rule list
            for all roecords in P reduce wieighting by decay factor
     				and adjust NP array accordingly
    	else prmGeneration(antecedent,c)
    	P' <-- tempP, N' <-- tempN', A' <-- tempA', NP' <-- tempNP'
end loop
</PRE>
</TD></TABLE>
<P><B>Table 2</B>: <I>The</I> <TT>prmGenerastion</TT> <I>method.</I></P>
<P><FONT SIZE="-1">1. An attribute is not in the antecedent if the appropriate 
cell in the attributes array  has the value <TT>0</TT>.</FONT></P>
</CENTER>

<P>In their experiments Xiaoxin Yin and Jiawei Han used a minimum gain constant
of <TT>0.7</TT>, and a decay factor of <TT>1/3</TT>. New weightings are 
thus calculated as follows:</P>

<PRE>
newWeighting = oldWeighting*decayFractor
</PRE>

<P>and gain is calculated as follows:</P>

<PRE>
WP  = totalWeight(P)
WN  = totalWeight(N)
WP' = totalWeight(P')
WN' = totalWeight(N')
gain(a) = WP' (log(WP'/WP'+NP') - log(WP/WP+NP))
</PRE> 

<P>The minimum weight threshold for <TT>P</TT> is calculated by multiplying the start
weight of <TT>P</TT> by the <TT>TOTAL_WEIGHT_THRESHOLD</TT> which was set to 
<TT>0.05</TT> during experimentation. The <TT>GAIN_SIMILARITY_RATIO</TT> was 
set to <TT>0.99</TT>.</P>

<BR><HR><BR>
<A NAME = "pruning">
<TABLE BORDER-=0 WIDTH="100%" CELLPADDING=5 BGCOLOR="006699">
<TR><TD><H2><font color = "white">2. RULE PRUNING</font></H2></TD>
</TABLE><BR>

<P>It is possible, using CPAR, to generate two or more rules with identical 
antecedents and consequents. This is because records are not removed from the 
N and P lists once a rule that satisfies them has been found, but instead 
these records simply have their weughting reduced. This reduced weighting forms 
part of the algorithm to calculate gains, however it is still possible for the 
same attributes to be selected to form the antecedent of a rule because these
attributes (despite the reduce weighting) still produce the best gain.
Eventually the weighting for the effected records is reduced so far that the
attributes do not produce a best gain and are therefore not selected. 
Where this occurs the rules with the lower accuracy are removed from
the rule list.</P>

<BR><HR><BR>
<A NAME = "testing">
<TABLE BORDER-=0 WIDTH="100%" CELLPADDING=5 BGCOLOR="006699">
<TR><TD><H2><font color = "white">3. TESTING</font></H2></TD>
</TABLE><BR>

<P>During testing, for each record in the test sets (as recommended in Yin and
Han 2003) the record is classified using the following procedure:</P>

<OL>
<LI>Obtain all rules whose antecedent is a subset of the given record.
<LI>Select the best <TT>K</TT> rules for each class according to their Laplace
accuracy. In experiments <TT>K</TT> was set to <TT>5</TT>).
<LI>Determine the average expected accuracy over the selected rules for each
class
<LI>Select the class with the best average expected accuracy.
</OL>

<BR><HR><BR>
<A NAME = "example">
<TABLE BORDER-=0 WIDTH="100%" CELLPADDING=5 BGCOLOR="006699">
<TR><TD><H2><font color = "white">4. WORKED EXAMPLE</font></H2></TD>
</TABLE>

<P>Note that for illustrative purposes, in the following example, the
<TT>GAIN_SIMILARITY_RATIO</TT> has been set to 0.6 instead of a more
usual value of about 0.99.</P>

<TABLE>
<TR><TD>
<FONT SIZE=-1>
<P>given the following data set (first 15 lines from the Pima Indian set):</P>

<PRE>
2 9 13 17 21 28 32 38 42 
1 8 13 17 21 27 31 36 41 
3 10 13 16 21 27 32 36 42 
1 8 13 17 21 28 31 36 41 
1 9 12 17 21 29 35 37 42 
2 8 14 16 21 27 31 36 41 
1 7 13 17 21 28 31 36 42 
3 8 11 16 21 28 31 36 41 
1 10 13 18 24 28 31 38 42 
3 9 14 16 21 26 31 38 42 
2 8 14 16 21 28 31 36 41 
3 10 14 16 21 28 31 37 42 
3 9 14 16 21 28 33 39 41 
1 10 13 17 25 28 31 39 42 
2 10 13 16 22 27 32 38 42 
</PRE>

<P>We commence with class <TT>41</TT> and create the following P and N sets 
and an NP array. The first column numbers in the NP array gives the attribute
label (1 to 40), the following two columns give the positive and negative
total weightings for the P and N example sets respectively. On "start up"
each attribute has a weighting of 1 associated with it, so attribute 1 occurs
twice in P and four times in N.</P>

<PRE>
positiveExamples:
 {1 8 13 17 21 27 31 36 41}  (1.0)
 {1 8 13 17 21 28 31 36 41}  (1.0)
 {2 8 14 16 21 27 31 36 41}  (1.0)
 {3 8 11 16 21 28 31 36 41}  (1.0)
 {2 8 14 16 21 28 31 36 41}  (1.0)
 {3 9 14 16 21 28 33 39 41}  (1.0)

negativeExamples:
 {2 9 13 17 21 28 32 38 42}  (1.0)
 {3 10 13 16 21 27 32 36 42}  (1.0)
 {1 9 12 17 21 29 35 37 42}  (1.0)
 {1 7 13 17 21 28 31 36 42}  (1.0)
 {1 10 13 18 24 28 31 38 42}  (1.0)
 {3 9 14 16 21 26 31 38 42}  (1.0)
 {3 10 14 16 21 28 31 37 42}  (1.0)
 {1 10 13 17 25 28 31 39 42}  (1.0)
 {2 10 13 16 22 27 32 38 42}  (1.0)

PN array 2
Att 1: 	2.0 	4.0
Att 2: 	2.0 	2.0
Att 3: 	2.0 	3.0
Att 4: 	0.0 	0.0
Att 5: 	0.0 	0.0
Att 6: 	0.0 	0.0
Att 7: 	0.0 	1.0
Att 8: 	5.0 	0.0
Att 9: 	1.0 	3.0
Att 10: 0.0 	5.0
Att 11: 1.0 	0.0
Att 12: 0.0 	1.0
Att 13: 2.0 	6.0
Att 14: 3.0 	2.0
Att 15: 0.0 	0.0
Att 16: 4.0 	4.0
Att 17: 2.0 	4.0
Att 18: 0.0 	1.0
Att 19: 0.0 	0.0
Att 20: 0.0 	0.0
Att 21: 6.0 	6.0
Att 22: 0.0 	1.0
Att 23: 0.0 	0.0
Att 24: 0.0 	1.0
Att 25: 0.0 	1.0
Att 26: 0.0 	1.0
Att 27: 2.0 	2.0
Att 28: 4.0 	5.0
Att 29: 0.0 	1.0
Att 30: 0.0 	0.0
Att 31: 5.0 	5.0
Att 32: 0.0 	3.0
Att 33: 1.0 	0.0
Att 34: 0.0 	0.0
Att 35: 0.0 	1.0
Att 36: 5.0 	2.0
Att 37: 0.0 	2.0
Att 38: 0.0 	4.0
Att 39: 1.0 	1.0
Att 40: 0.0 	0.0
</PRE>

<P>This means that the total weight  of <TT>P</TT> is <TT>6.0</TT> and the
Total Weight Threshold (TWT) is:</P>

<PRE>
TWT = totalWeight*TOTAL_WEIGHT_THRESHOLD
    = 6.0*0.05 = 0.3
</PRE>
		     
<P>The set <TT>P</TT> is then processed, for the given class until its total 
weight falls below the threshold (<TT>0.3</TT>). First we make copies of 
<TT>P</TT>, <TT>N</TT>, <TT>A</TT> and <TT>PN</TT>: <TT>P'</TT>, 
<TT>N'</TT>, <TT>A'</TT> and <TT>PN'</TT>. We then claculate the gain (using threshold values) for each 
attribute were it to be added to the rule antecedent (which currently is empty) 
and place the resulting gains in the attributes array. The result is as 
follows:<P>

<PRE>
Atributes:
Col 1: 0.0 -0.36
Col 2: 0.0 0.45 
Col 3: 0.0 0.0 
Col 4: 0.0 0.0 
Col 5: 0.0 0.0 
Col 6: 0.0 0.0 
Col 7: 0.0 0.0 
Col 8: 0.0 4.58 
Col 9: 0.0 -0.47 
Col 10: 0.0 0.0 
Col 11: 0.0 0.91 
Col 12: 0.0 0.0 
Col 13: 0.0 -0.94 
Col 14: 0.0 1.21 
Col 15: 0.0 0.0 
Col 16: 0.0 0.89 
Col 17: 0.0 -0.36 
Col 18: 0.0 0.0 
Col 19: 0.0 0.0 
Col 20: 0.0 0.0 
Col 21: 0.0 1.33 
Col 22: 0.0 0.0 
Col 23: 0.0 0.0 
Col 24: 0.0 0.0 
Col 25: 0.0 0.0 
Col 26: 0.0 0.0 
Col 27: 0.0 0.45 
Col 28: 0.0 0.42 
Col 29: 0.0 0.0 
Col 30: 0.0 0.0 
Col 31: 0.0 1.12 
Col 32: 0.0 0.0 
Col 33: 0.0 0.92 
Col 34: 0.0 0.0 
Col 35: 0.0 0.0 
Col 36: 0.0 2.90 
Col 37: 0.0 0.0 
Col 38: 0.0 0.0 
Col 39: 0.0 0.22
Col 40: 0.0 0.0 
</PRE>

<P>As in the LUCS KDD implementation of the CPAR algorithm (and FOIL and PRM) 
the first digit in the attributes 2-D array is set to <TT>0.0</TT> to indicate 
that the attribute is available to
be included in a rule (i.e. not already in the current rule antecedent) and the
second to hold gain values when calculated. When an attribute becomes no longer
available the first digit will be set to <TT>1.0</TT>. The attribute array is used 
purely to enhance the computational efficiency of the algorithm.</P>

<P>If there were no gain above the user specified minimum we would add the rule
sofar to the rule list. However there are attributes (see above) with a gain 
above the minimum. The attribute with the best gain is attribute <TT>8</TT>
(gain = <TT>4.58</TT>). So the Local Gain Threshold (LGT) will be:

<PRE>
LGT = 4.58*GAIN_SIMILARITY_RATIO
    = 4.58*0.6
    = 2.75
</PRE>

<P>There are actually two attributes above this local minimum: <TT>8</TT> and 
<TT>36</TT>.</P>		     
		     
<P>We process the attributes array attribute by attribute looking for gains
above the local minimum and will arrive at the attribute <TT>8</TT> first. We 
make copies of <TT>P'</TT>, <TT>N'</TT>, <TT>A'</TT> and <TT>PN'</TT>; and 
proceed. with these copies, as follows:</P>

<OL>
<LI>Add attribute to the rule antecedent <TT>8 -> 41</TT>.
<LI>Adjusted the <TT>P'</TT> and <TT>N'</TT> copies so that all examples which 
do not contain attribute <TT>8</TT> are removed and the copy of the <TT>PN</TT>
array adjusted accordingly.
</OL>

<P>Thus:</P>

<PRE>
positiveExamples2 (copy):
 {1 8 13 17 21 27 31 36 41} 
 {1 8 13 17 21 28 31 36 41} 
 {2 8 14 16 21 27 31 36 41} 
 {3 8 11 16 21 28 31 36 41} 
 {2 8 14 16 21 28 31 36 41} 

negativeExamples2 (copy): Null

PN array 2 (copy):
Att 1: 	2.0 	0.0
Att 2: 	2.0 	0.0
Att 3: 	1.0 	0.0
Att 4: 	0.0 	0.0
Att 5: 	0.0 	0.0
Att 6: 	0.0 	0.0
Att 7: 	0.0 	0.0
Att 8: 	5.0 	0.0
Att 9: 	0.0 	0.0
Att 10: 0.0 	0.0
Att 11: 1.0 	0.0
Att 12: 0.0 	0.0
Att 13: 2.0 	0.0
Att 14: 2.0 	0.0
Att 15: 0.0 	0.0
Att 16: 3.0 	0.0
Att 17: 2.0 	0.0
Att 18: 0.0 	0.0
Att 19: 0.0 	0.0
Att 20: 0.0 	0.0
Att 21: 5.0 	0.0
Att 22: 0.0 	0.0
Att 23: 0.0 	0.0
Att 24: 0.0 	0.0
Att 25: 0.0 	0.0
Att 26: 0.0 	0.0
Att 27: 2.0 	0.0
Att 28: 3.0 	0.0
Att 29: 0.0 	0.0
Att 30: 0.0 	0.0
Att 31: 5.0 	0.0
Att 32: 0.0 	0.0
Att 33: 0.0 	0.0
Att 34: 0.0 	0.0
Att 35: 0.0 	0.0
Att 36: 5.0 	0.0
Att 37: 0.0 	0.0
Att 38: 0.0 	0.0
Att 39: 0.0 	0.0
Att 40: 0.0 	0.0
</PRE>

<P><TT>N'</TT> is now empty thus the rule <TT>8 -> 41</TT> is inserted into
the rule list. We revise the weightings in <TT>P by the decay factor and adjust
the <TT>PN</TT> array accordingly. This will give a <TT>P</TT> example set of:</P>

</FONT>
</TD><TD><PRE> </PRE></TD><TD>
<FONT SIZE=-1>

<PRE>
positiveExamples:
 {1 8 13 17 21 27 31 36 41}  (0.33)
 {1 8 13 17 21 28 31 36 41}  (0.33)
 {2 8 14 16 21 27 31 36 41}  (0.33)
 {3 8 11 16 21 28 31 36 41}  (0.33)
 {2 8 14 16 21 28 31 36 41}  (0.33)
 {3 9 14 16 21 28 33 39 41}  (1.0) 
</PRE>

<P>We return to the attribute list, and continue processing looking for
attributes with a gain above the minimum of <TT>2.75</TT>, and find attribute 
<TT>36</TT> (gain = <TT>2.899092476264711</TT>). We add this rule to the 
antecedent so far to give <TT>36 -> 41</TT> and continue processing with
copies of <TT>N'</TT>, <TT>P'</TT>, <TT>A'</TT> and <TT>NP'</TT>.</P>

<P>The copies of <TT>N'</TT> and <TT>P'</TT> are reduced and the copu of 
<TT>NP'</TT> adjusted to give:</P>

<PRE>
positiveExamples2 (copy):
 {1 8 13 17 21 27 31 36 41}  (1.0)
 {1 8 13 17 21 28 31 36 41}  (1.0)
 {2 8 14 16 21 27 31 36 41}  (1.0)
 {3 8 11 16 21 28 31 36 41}  (1.0)
 {2 8 14 16 21 28 31 36 41}  (1.0)

negativeExamples2 (copy):
Null

NP array (copy):
Att 1: 	2.0 	0.0
Att 2: 	2.0 	0.0
Att 3: 	1.0 	0.0
Att 4: 	0.0 	0.0
Att 5: 	0.0 	0.0
Att 6: 	0.0 	0.0
Att 7: 	0.0 	0.0
Att 8: 	5.0 	0.0
Att 9: 	0.0 	0.0
Att 10: 0.0 	0.0
Att 11: 1.0 	0.0
Att 12: 0.0 	0.0
Att 13: 2.0 	0.0
Att 14: 2.0 	0.0
Att 15: 0.0 	0.0
Att 16: 3.0 	0.0
Att 17: 2.0 	0.0
Att 18: 0.0 	0.0
Att 19: 0.0 	0.0
Att 20: 0.0 	0.0
Att 21: 5.0 	0.0
Att 22: 0.0 	0.0
Att 23: 0.0 	0.0
Att 24: 0.0 	0.0
Att 25: 0.0 	0.0
Att 26: 0.0 	0.0
Att 27: 2.0 	0.0
Att 28: 3.0 	0.0
Att 29: 0.0 	0.0
Att 30: 0.0 	0.0
Att 31: 5.0 	0.0
Att 32: 0.0 	0.0
Att 33: 0.0 	0.0
Att 34: 0.0 	0.0
Att 35: 0.0 	0.0
Att 36: 5.0 	0.0
Att 37: 0.0 	0.0
Att 38: 0.0 	0.0
Att 39: 0.0 	0.0
Att 40: 0.0 	0.0
</PRE>

<P>The copy of <TT>N'</TT> is now empty so the rule <TT>36 -> 41</TT> as inserted 
into the rule list and <TT>P</TT> adjusted accordingly:<P>

<PRE>
positiveExamples:
{1 8 13 17 21 27 31 36 41}  (0.11)
{1 8 13 17 21 28 31 36 41}  (0.11)
{2 8 14 16 21 27 31 36 41}  (0.11)
{3 8 11 16 21 28 31 36 41}  (0.11)
{2 8 14 16 21 28 31 36 41}  (0.11)
{3 9 14 16 21 28 33 39 41}  (1.0)
</PRE>

<P><TT>P</TT> has now had all the weightings that satisfy the rules 
<TT>8 -> 41</TT> and <TT>36->41</TT> reduced by the given decay factor of 
<TT>1/3</TT>. The <TT>NP</TT> array is now:</P>

<PRE>
Att 1: 	0.22	4.0
Att 2: 	0.22 	2.0
Att 3: 	1.11	3.0
Att 4: 	0.0 	0.0
Att 5: 	0.0 	0.0
Att 6: 	0.0 	0.0
Att 7: 	0.0 	1.0
Att 8: 	0.55 	0.0
Att 9: 	1.0 	3.0
Att 10: 0.0 	5.0
Att 11: 0.11 	0.0
Att 12: 0.0 	1.0
Att 13: 0.22	6.0
Att 14: 1.22	2.0
Att 15: 0.0 	0.0
Att 16: 1.33 	4.0
Att 17: 0.22 	4.0
Att 18: 0.0 	1.0
Att 19: 0.0 	0.0
Att 20: 0.0 	0.0
Att 21: 1.55 	6.0
Att 22: 0.0 	1.0
Att 23: 0.0 	0.0
Att 24: 0.0 	1.0
Att 25: 0.0 	1.0
Att 26: 0.0 	1.0
Att 27: 0.22 	2.0
Att 28: 1.33 	5.0
Att 29: 0.0 	1.0
Att 30: 0.0 	0.0
Att 31: 0.55 	5.0
Att 32: 0.0 	3.0
Att 33: 1.0 	0.0
Att 34: 0.0 	0.0
Att 35: 0.0 	1.0
Att 36: 0.55 	2.0
Att 37: 0.0 	2.0
Att 38: 0.0 	4.0
Att 39: 1.0 	1.0
Att 40: 0.0 	0.0
</PRE>

<P>The Total Weight Threshold (TWT) for <TT>P</TT> is now <TT>1.55</TT>, above 
the minimum threshold of <TT>0.3</TT>, so we repeat the process with the 
revised example sets <TT>P'</TT> (copied from the revised version of 
<TT>P</TT>) and <TT>N'</TT> (copied from the original <TT>N</TT>):</P>

<PRE>
positiveExamples2:
 {1 8 13 17 21 27 31 36 41}  (0.11)
 {1 8 13 17 21 28 31 36 41}  (0.11)
 {2 8 14 16 21 27 31 36 41}  (0.11)
 {3 8 11 16 21 28 31 36 41}  (0.11)
 {2 8 14 16 21 28 31 36 41}  (0.11)
 {3 9 14 16 21 28 33 39 41}  (1.0)

negativeExamples2:
 {2 9 13 17 21 28 32 38 42}  (1.0)
 {3 10 13 16 21 27 32 36 42}  (1.0)
 {1 9 12 17 21 29 35 37 42}  (1.0)
 {1 7 13 17 21 28 31 36 42}  (1.0)
 {1 10 13 18 24 28 31 38 42}  (1.0)
 {3 9 14 16 21 26 31 38 42}  (1.0)
 {3 10 14 16 21 28 31 37 42}  (1.0)
 {1 10 13 17 25 28 31 39 42}  (1.0)
 {2 10 13 16 22 27 32 38 42}  (1.0)
</PRE>

<P>We also produce a new <TT>NP'</TT> made of the positive weightings from the
previous <TT>NP'</TT> and the negative weightings from <TT>NP</TT>. The result
is as follows:

<PRE>
Att 1: 	0.22 	4.0
Att 2: 	0.22 	2.0
Att 3: 	1.11 	3.0
Att 4: 	0.0 	0.0
Att 5: 	0.0 	0.0
Att 6: 	0.0 	0.0
Att 7: 	0.0 	1.0
Att 8: 	0.56 	0.0
Att 9: 	1.0 	3.0
Att 10: 0.0 	5.0
Att 11: 0.11 	0.0
Att 12: 0.0 	1.0
Att 13: 0.22 	6.0
Att 14: 1.22 	2.0
Att 15: 0.0 	0.0
Att 16: 1.33 	4.0
Att 17: 0.22 	4.0
Att 18: 0.0 	1.0
Att 19: 0.0 	0.0
Att 20: 0.0 	0.0
Att 21: 1.56 	6.0
Att 22: 0.0 	1.0
Att 23: 0.0 	0.0
Att 24: 0.0 	1.0
Att 25: 0.0 	1.0
Att 26: 0.0 	1.0
Att 27: 0.22 	2.0
Att 28: 1.33 	5.0
Att 29: 0.0 	1.0
Att 30: 0.0 	0.0
Att 31: 0.56 	5.0
Att 32: 0.0 	3.0
Att 33: 1.0 	0.0
Att 34: 0.0 	0.0
Att 35: 0.0 	1.0
Att 36: 0.56 	2.0
Att 37: 0.0 	2.0
Att 38: 0.0 	4.0
Att 39: 1.0 	1.0
Att 40: 0.0 	0.0
</PRE>

<P>we also create an empty attributes array <TT>A'</TT> (by copying 
<TT>A</TT>). As before we then calculate all gains for each attribute (and 
store in the attributes array). The attribute with the best gain is now 
attribute <TT>33</TT> (gain = <TT>0.92</TT>) which means the Local Gain 
Threshold (LGT) will be calculated as:</P>

<PRE>
LGT = 0.92*GAIN_SIMILARITY_RATIO
    = 0.92*0.6
    = 0.55
</PRE>

<P>Which is above specified global minimum of <TT>0.7</TT>, so <TT>0.7</TT> 
will be used as the 
local minimum. There is only one attribute above this threshold (<TT>33</TT>).
This is inserted into the rule antecedent to give <TT>33 -> 41</TT> and the
copies of the <TT>P'</TT> and <TT>N'</TT> example sets and the copy of the 
<TT>NP'</TT> array adjusted accordingly. <TT>N'</TT> is now empty 
so the rule <TT>33 -> 41</TT> is inserted into the rule list.</P>

<P>We now repeat again. The total weighting for <TT>P</TT> is now
<TT>0.89</TT>, therefore above the totalWeightThreshold of 
<TT>0.3</TT>. We check that it is possible to produce a gain above the global
maximum and discover that this cannot be done so break our of the loop
and repeat the entire process for class <TT>42</TT>.</P> 

<P>At the end of the process we will have the following rule list. Note that
rules are inserted into the rule list, as they are derived according to their
calculate Laplace accuracy (derived according to each rules Laplace expected 
error estimate).</P>

<PRE>
(1)  {8}   ->  {41}  0.86%
(2)  {10}  ->  {42}  0.86%
(3)  {38}  ->  {42}  0.83%
(4)  {32}  ->  {42}  0.8%
(5)  {36}  ->  {41}  0.67%
(6)  {33}  ->  {41}  0.67%
</PRE>
</FONT>
</TD></TABLE>

<br><hr><br>
<A name = "conclusions">
<table BORDER-=0 WIDTH="100%" CELLPADDING=5 BGCOLOR="006699">
<tr><td><h2><font color =" white">5. CONCLUSSIONS</font></h2></td>
</table> <BR>

<P>The LUCS-KDD implementation of CPAR described here has been use by the 
LUCS-KDD reseach group to contrast and compare a variety of CARM algorithms 
and techniques. The software is available free of charge, however the author 
would appreciate appropriate acknowledgement. The following reference format 
for referring to the software is suggested:</P>

<P>Coenen, F. (2004), <I>LUCS-KDD implementations of CPR
(Classification based on Predictive Association Rules)</I>,
http://www.cxc.liv.ac.uk/~frans/KDD/Software/FOIL_PRM_CPAR/cpar.html,
Department of Computer Science, The University of Liverpool, UK.</P>

<P>Should you discover any "bugs" or other problems within the software (or this 
documentation), do not hesitate to contact 
the author.</P>

<BR><HR><BR>
<H1>REFERENCES</H1>
<OL>
<LI>Clark, P. and Boswell. R. (1991).
<I>Rule Induction With CN2: Some Recent Improvements.</I>
Proc. European Working Session on Learning (ESWL'91), Porto, Portugal.
pp151-163.
<LI>Quinlan, J. R. and Cameron-Jones, R. M. (1993).
<I>FOIL: A Midterm Report.</I>
Proc. ECML, Vienna, Austria, pp3-20.
<LI>Yin, X. and Han, J. (2003).
<I>CPAR: Classification based on Predictive Association Rules.</I>
Proc. SIAM Int. Conf. on Data Mining (SDM'03), San Fransisco, CA, pp. 331-335.
</OL>

<BR><HR><BR>
<P>Created and maintained by
<A HREF=http://www.csc.liv.ac.uk/~frans/>Frans Coenen</A>.
<!--#config timefmt="%d %B %Y"-->Last updated <!--#echo var="LAST_MODIFIED"-->
</BODY>
</HTML>

